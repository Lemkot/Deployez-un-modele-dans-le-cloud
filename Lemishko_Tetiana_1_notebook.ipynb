{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce56a14",
   "metadata": {},
   "source": [
    "FRUITS wants to develop a mobile application that would allow users to take a picture of a fruit and obtain information about this fruit. This application would make it possible to set up a first version of the fruit image classification engine.\n",
    "\n",
    "Our mission is therefore to develop in a Big Data (AWS) environment a first data processing chain which will include preprocessing and a dimension reduction step (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a6ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark initialisation\n",
    "import findspark\n",
    "findspark.init(\"/home/bitnami/spark-3.0.0-bin-hadoop3.2\")\n",
    "import pyspark\n",
    "import pyarrow\n",
    "# context & session\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# usefull packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "# deal with image\n",
    "from PIL import Image\n",
    "# data handling\n",
    "from pyspark.sql.functions import element_at, split, col,size\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window\n",
    "from typing import Iterator\n",
    "# ml tasks\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "# transform\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "# core featurizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import io\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37182ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71a6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credential_pickle.pickle', 'rb') as file:\n",
    "    credential = pickle.Unpickler(file).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97dd808",
   "metadata": {},
   "source": [
    "### Spark configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c5a9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f0bb94c4cd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf=SparkConf()\n",
    "conf.set('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.2.0')\n",
    "conf.set('spark.executor.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true')\n",
    "conf.set('spark.driver.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true')\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.access.key\", credential['AWS_ACCESS_KEY_ID'])\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.secret.key\", credential['AWS_SECRET_ACCESS_KEY'])\n",
    "conf.set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "conf.set('fs.s3a.endpoint', \"s3-eu-west-3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c9247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.endpoint\n",
      "22/12/06 19:41:38 WARN Utils: Your hostname, ip-172-31-45-74 resolves to a loopback address: 127.0.0.1; using 172.31.45.74 instead (on interface eth0)\n",
      "22/12/06 19:41:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bitnami/spark-3.0.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/bitnami/.ivy2/cache\n",
      "The jars for the packages stored in: /home/bitnami/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/bitnami/spark-3.0.0-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f09c3ee6-30ee-4512-86f9-01712fdccea9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      ":: resolution report :: resolve 521ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f09c3ee6-30ee-4512-86f9-01712fdccea9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/13ms)\n",
      "22/12/06 19:41:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).appName('Projet8_new').getOrCreate()\n",
    "spark._sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1fe7361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://provisioner-local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Projet8_new</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0bb83f39a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9dedd",
   "metadata": {},
   "source": [
    "### AWS S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40619bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7003063",
   "metadata": {},
   "source": [
    "Let's define a function to display the present s3 buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5122d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_buckets(ressource):\n",
    "    \"\"\"Print all existing buckets names\"\"\"\n",
    "    for bucket in ressource.buckets.all():\n",
    "        print(bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ff44e",
   "metadata": {},
   "source": [
    "Show the buckets present in S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff3a708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-bucket-lemishkot\n"
     ]
    }
   ],
   "source": [
    "list_buckets(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5486c",
   "metadata": {},
   "source": [
    "## Pre-processing of the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df9bc0",
   "metadata": {},
   "source": [
    "### Load categories of the fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e317c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_categories(bucket, prefix):\n",
    "    \"\"\"Function to create a list of all directories containing the image.\"\"\"\n",
    "    client=boto3.client('s3')\n",
    "    sub_folders =[]\n",
    "    result=client.list_objects(Bucket=bucket ,Prefix=prefix, Delimiter='/')\n",
    "    for o in result.get('CommonPrefixes'):\n",
    "        sub_folders.append(list(o.get('Prefix').split(\" \")))\n",
    "    print(\"Number of images's category is:\", len(sub_folders))\n",
    "    return sub_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dedf6e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images's category is: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['sample/Apricot/'],\n",
       " ['sample/Avocado/'],\n",
       " ['sample/Banana/'],\n",
       " ['sample/Blueberry/'],\n",
       " ['sample/Strawberry/']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket='test-bucket-lemishkot'\n",
    "prefix=\"sample/\"\n",
    "list_cat = list_categories(bucket, prefix)\n",
    "list_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d274d9",
   "metadata": {},
   "source": [
    "**Test - Random choice of a fruit category:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6d2a64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample/Banana/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_category = random.choice(list_cat)[0]\n",
    "fruit_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389657db",
   "metadata": {},
   "source": [
    "### Loading of the images and extraction of the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3837415",
   "metadata": {},
   "source": [
    "S3 path of the images of the chosen category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb0e56fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://test-bucket-lemishkot/sample/Banana/'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"s3a://test-bucket-lemishkot/\" + fruit_category\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "559ef598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/06 19:41:47 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+------+\n",
      "|                path|   modificationTime|length|             content| label|\n",
      "+--------------------+-------------------+------+--------------------+------+\n",
      "|s3a://test-bucket...|2022-11-28 14:18:56|  3643|[FF D8 FF E0 00 1...|Banana|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3583|[FF D8 FF E0 00 1...|Banana|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3574|[FF D8 FF E0 00 1...|Banana|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3484|[FF D8 FF E0 00 1...|Banana|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3425|[FF D8 FF E0 00 1...|Banana|\n",
      "+--------------------+-------------------+------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_df = spark.read.format(\"binaryFile\").option(\"pathGlobFilter\", \"*.jpg\").load(path).cache()\n",
    "images_df = images_df.withColumn('label', element_at(split(col('path'), '/'),-2))\n",
    "images_df.printSchema()\n",
    "images_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa5eb3",
   "metadata": {},
   "source": [
    "### Creation of image features via CNN Transfer Learning (VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5448b0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 19:42:03.592748: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-06 19:42:03.597250: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-06 19:42:03.600428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-45-74): /proc/driver/nvidia/version does not exist\n",
      "2022-12-06 19:42:03.627110: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-06 19:42:04.717842: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 9437184 exceeds 10% of free system memory.\n",
      "2022-12-06 19:42:04.737452: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 9437184 exceeds 10% of free system memory.\n",
      "2022-12-06 19:42:04.742470: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 9437184 exceeds 10% of free system memory.\n",
      "2022-12-06 19:42:04.765592: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 9437184 exceeds 10% of free system memory.\n",
      "2022-12-06 19:42:04.784571: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 9437184 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    pooling='max',\n",
    "    input_shape=(100, 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be3be834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 100, 100, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 100, 100, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 50, 50, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 50, 50, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 25, 25, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 25, 25, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 12, 12, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 512)              0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d16d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the extractions of the features in the form of list, which takes as the arguments the \n",
    "# fruit cathegory (see line 13) and the bucket\n",
    "def Featurizer(category,bucket):\n",
    "    \"\"\"Function to load the images of the chosen category\n",
    "    and extraction of features from images via the pre-trained model VGG16\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    feat = []\n",
    "    for file in bucket.objects.filter(Prefix=category):\n",
    "        #load image\n",
    "        obj = bucket.Object(file.key)\n",
    "        label = file.key.split('/')[-2]\n",
    "        response = obj.get()\n",
    "        file_stream = response['Body']\n",
    "        img = Image.open(file_stream)\n",
    "        # convert image to flatten array\n",
    "        flat_array = np.array(img).ravel().tolist()\n",
    "        tensor = np.array(flat_array).reshape(1, 100, 100, 3).astype(np.uint8)\n",
    "        #  vgg 16 preprocess input\n",
    "        prep_tensor = preprocess_input(tensor)\n",
    "        # vgg16 extraction features\n",
    "        features = model.predict(prep_tensor).ravel().tolist()\n",
    "        # Store file key, flat array and features\n",
    "        feat.append((features))\n",
    "    stop = time.perf_counter()\n",
    "    print(f'process, elapsed time: {stop - start:0.2f}s')\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fed542f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "process, elapsed time: 4.73s\n"
     ]
    }
   ],
   "source": [
    "fruits_bucket = s3.Bucket('test-bucket-lemishkot')\n",
    "features = Featurizer(fruit_category,fruits_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6dc14d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/06 19:42:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/06 19:42:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content| label|            features|\n",
      "+--------------------+-------------------+------+--------------------+------+--------------------+\n",
      "|s3a://test-bucket...|2022-11-28 14:18:56|  3643|[FF D8 FF E0 00 1...|Banana|[0.18677103519439...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3583|[FF D8 FF E0 00 1...|Banana|[0.21086037158966...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3574|[FF D8 FF E0 00 1...|Banana|[0.22115191817283...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3484|[FF D8 FF E0 00 1...|Banana|[0.19759744405746...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3425|[FF D8 FF E0 00 1...|Banana|[0.19242888689041...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:58|  3408|[FF D8 FF E0 00 1...|Banana|[0.19521124660968...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:58|  3348|[FF D8 FF E0 00 1...|Banana|[0.16757418215274...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:58|  3348|[FF D8 FF E0 00 1...|Banana|[0.15665827691555...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:58|  3235|[FF D8 FF E0 00 1...|Banana|[0.15315200388431...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:58|  3233|[FF D8 FF E0 00 1...|Banana|[0.16835924983024...|\n",
      "+--------------------+-------------------+------+--------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a new column in a dataframe with the extracted features\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "b = spark.createDataFrame([(l,) for l in features], ['features'])\n",
    "images_df = images_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "b = b.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "images_df = images_df.join(b, images_df.row_idx == b.row_idx).\\\n",
    "             drop(\"row_idx\")\n",
    "images_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ef8a0",
   "metadata": {},
   "source": [
    "### Dimension reduction via PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee87618",
   "metadata": {},
   "source": [
    "**Operation that transforms feature lists into vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9793d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf42597",
   "metadata": {},
   "source": [
    "**Convert features into vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08595a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectors = images_df.withColumn('vectors', array_to_vector_udf('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21fb9896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/06 19:42:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/06 19:42:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+\n",
      "|                path|   modificationTime|length|             content| label|            features|             vectors|\n",
      "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+\n",
      "|s3a://test-bucket...|2022-11-28 14:18:56|  3643|[FF D8 FF E0 00 1...|Banana|[0.18677103519439...|[0.18677103519439...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3583|[FF D8 FF E0 00 1...|Banana|[0.21086037158966...|[0.21086037158966...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3574|[FF D8 FF E0 00 1...|Banana|[0.22115191817283...|[0.22115191817283...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3484|[FF D8 FF E0 00 1...|Banana|[0.19759744405746...|[0.19759744405746...|\n",
      "|s3a://test-bucket...|2022-11-28 14:18:57|  3425|[FF D8 FF E0 00 1...|Banana|[0.19242888689041...|[0.19242888689041...|\n",
      "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_vectors.show(5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27713864",
   "metadata": {},
   "source": [
    "**PCA dimension reduction model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "686f0cd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/06 19:42:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/06 19:42:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/06 19:42:27 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/12/06 19:42:27 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/12/06 19:42:27 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "22/12/06 19:42:27 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(k=10, inputCol='vectors', outputCol='pca_vectors')\n",
    "model_pca = pca.fit(df_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07f9e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the model\n",
    "df_pca = model_pca.transform(df_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e045878",
   "metadata": {},
   "source": [
    "One should use the function limit in order to improve the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbfd493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/06 19:42:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/06 19:42:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/06 19:42:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/06 19:42:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th><th>modificationTime</th><th>length</th><th>content</th><th>label</th><th>features</th><th>vectors</th><th>pca_vectors</th></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:56</td><td>3643</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.18677103519439...</td><td>[0.18677103519439...</td><td>[1.80532790014871...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3583</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.21086037158966...</td><td>[0.21086037158966...</td><td>[1.76155586971014...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3574</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.22115191817283...</td><td>[0.22115191817283...</td><td>[1.80613388469232...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3484</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.19759744405746...</td><td>[0.19759744405746...</td><td>[1.84911991928906...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3425</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.19242888689041...</td><td>[0.19242888689041...</td><td>[1.92510207279949...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+--------------------+\n",
       "|                path|   modificationTime|length|             content| label|            features|             vectors|         pca_vectors|\n",
       "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+--------------------+\n",
       "|s3a://test-bucket...|2022-11-28 14:18:56|  3643|[FF D8 FF E0 00 1...|Banana|[0.18677103519439...|[0.18677103519439...|[1.80532790014871...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3583|[FF D8 FF E0 00 1...|Banana|[0.21086037158966...|[0.21086037158966...|[1.76155586971014...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3574|[FF D8 FF E0 00 1...|Banana|[0.22115191817283...|[0.22115191817283...|[1.80613388469232...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3484|[FF D8 FF E0 00 1...|Banana|[0.19759744405746...|[0.19759744405746...|[1.84911991928906...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3425|[FF D8 FF E0 00 1...|Banana|[0.19242888689041...|[0.19242888689041...|[1.92510207279949...|\n",
       "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+--------------------+"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "df_pca.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bc733e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Transformation (from Vectors to Arrays)\n",
    "vector_to_array_udf = udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3b850d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_pca.withColumn('pca_features', vector_to_array_udf('pca_vectors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee865d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/06 19:42:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/06 19:42:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/06 19:42:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/06 19:42:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th><th>modificationTime</th><th>length</th><th>content</th><th>label</th><th>features</th><th>vectors</th><th>pca_vectors</th><th>pca_features</th></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:56</td><td>3643</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.18677103519439...</td><td>[0.18677103519439...</td><td>[1.80532790014871...</td><td>[1.8053279, -0.10...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3583</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.21086037158966...</td><td>[0.21086037158966...</td><td>[1.76155586971014...</td><td>[1.7615559, -0.14...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3574</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.22115191817283...</td><td>[0.22115191817283...</td><td>[1.80613388469232...</td><td>[1.8061339, -0.18...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3484</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.19759744405746...</td><td>[0.19759744405746...</td><td>[1.84911991928906...</td><td>[1.8491199, -0.23...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3425</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.19242888689041...</td><td>[0.19242888689041...</td><td>[1.92510207279949...</td><td>[1.9251021, -0.32...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+--------------------+--------------------+\n",
       "|                path|   modificationTime|length|             content| label|            features|             vectors|         pca_vectors|        pca_features|\n",
       "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+--------------------+--------------------+\n",
       "|s3a://test-bucket...|2022-11-28 14:18:56|  3643|[FF D8 FF E0 00 1...|Banana|[0.18677103519439...|[0.18677103519439...|[1.80532790014871...|[1.8053279, -0.10...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3583|[FF D8 FF E0 00 1...|Banana|[0.21086037158966...|[0.21086037158966...|[1.76155586971014...|[1.7615559, -0.14...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3574|[FF D8 FF E0 00 1...|Banana|[0.22115191817283...|[0.22115191817283...|[1.80613388469232...|[1.8061339, -0.18...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3484|[FF D8 FF E0 00 1...|Banana|[0.19759744405746...|[0.19759744405746...|[1.84911991928906...|[1.8491199, -0.23...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3425|[FF D8 FF E0 00 1...|Banana|[0.19242888689041...|[0.19242888689041...|[1.92510207279949...|[1.9251021, -0.32...|\n",
       "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+--------------------+--------------------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2002f8b7",
   "metadata": {},
   "source": [
    "### Save resulting file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ec654d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/06 19:42:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/06 19:42:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving the final df in parquet format into S3 bucket\n",
    "final_df.write.format(\"parquet\").mode('overwrite').save('s3a://test-bucket-lemishkot/results_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d478da",
   "metadata": {},
   "source": [
    "### Read and charge the resulting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a75b9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_parquet = spark.read.format('parquet').load('s3a://test-bucket-lemishkot/results_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fc70aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th><th>modificationTime</th><th>length</th><th>content</th><th>label</th><th>features</th><th>vectors</th><th>pca_vectors</th><th>pca_features</th></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:56</td><td>3643</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.18677103519439...</td><td>[0.18677103519439...</td><td>[1.80532790014871...</td><td>[1.8053279, -0.10...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3583</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.21086037158966...</td><td>[0.21086037158966...</td><td>[1.76155586971014...</td><td>[1.7615559, -0.14...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3574</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.22115191817283...</td><td>[0.22115191817283...</td><td>[1.80613388469232...</td><td>[1.8061339, -0.18...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3484</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.19759744405746...</td><td>[0.19759744405746...</td><td>[1.84911991928906...</td><td>[1.8491199, -0.23...</td></tr>\n",
       "<tr><td>s3a://test-bucket...</td><td>2022-11-28 14:18:57</td><td>3425</td><td>[FF D8 FF E0 00 1...</td><td>Banana</td><td>[0.19242888689041...</td><td>[0.19242888689041...</td><td>[1.92510207279949...</td><td>[1.9251021, -0.32...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+--------------------+--------------------+\n",
       "|                path|   modificationTime|length|             content| label|            features|             vectors|         pca_vectors|        pca_features|\n",
       "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+--------------------+--------------------+\n",
       "|s3a://test-bucket...|2022-11-28 14:18:56|  3643|[FF D8 FF E0 00 1...|Banana|[0.18677103519439...|[0.18677103519439...|[1.80532790014871...|[1.8053279, -0.10...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3583|[FF D8 FF E0 00 1...|Banana|[0.21086037158966...|[0.21086037158966...|[1.76155586971014...|[1.7615559, -0.14...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3574|[FF D8 FF E0 00 1...|Banana|[0.22115191817283...|[0.22115191817283...|[1.80613388469232...|[1.8061339, -0.18...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3484|[FF D8 FF E0 00 1...|Banana|[0.19759744405746...|[0.19759744405746...|[1.84911991928906...|[1.8491199, -0.23...|\n",
       "|s3a://test-bucket...|2022-11-28 14:18:57|  3425|[FF D8 FF E0 00 1...|Banana|[0.19242888689041...|[0.19242888689041...|[1.92510207279949...|[1.9251021, -0.32...|\n",
       "+--------------------+-------------------+------+--------------------+------+--------------------+--------------------+--------------------+--------------------+"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_parquet.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f07d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
